<![CDATA[<div align="center">

# ğŸ® MPE Multi-Agent Benchmark

### LLM-Driven Multi-Agent Particle Environment Benchmark Suite

**å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒåŸºå‡†æµ‹è¯•å¥—ä»¶**

[![PettingZoo](https://img.shields.io/badge/PettingZoo-MPE-blue)](https://pettingzoo.farama.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

<!-- ğŸ–¼ï¸ PLACEHOLDER: é¡¹ç›®ä¸»æ¨ªå¹…å›¾ / Project Banner Image -->
<!-- ![Project Banner](./assets/banner.png) -->
<!-- å»ºè®®æ”¾ç½®: ä¸€å¼ åŒ…å«å¤šä¸ªæ¸¸æˆæˆªå›¾æ‹¼æ¥çš„æ¨ªå¹…å›¾, å°ºå¯¸ 1200x400 -->

[English](#english-version) Â· [ä¸­æ–‡](#ä¸­æ–‡ç‰ˆæœ¬) Â· [Quick Start](#-å¿«é€Ÿå¼€å§‹--quick-start) Â· [Games](#-æ¸¸æˆç¯å¢ƒæ€»è§ˆ--game-environments-overview)

</div>

---

<a name="ä¸­æ–‡ç‰ˆæœ¬"></a>

# ğŸ“– ä¸­æ–‡æ–‡æ¡£

## ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#-é¡¹ç›®æ¦‚è¿°)
- [ç³»ç»Ÿæ¶æ„](#-ç³»ç»Ÿæ¶æ„)
- [æ¸¸æˆç¯å¢ƒæ€»è§ˆ](#-æ¸¸æˆç¯å¢ƒæ€»è§ˆ)
- [å„æ¸¸æˆè¯¦è§£](#-å„æ¸¸æˆè¯¦è§£)
  - [1. Simple (å¯¼èˆª)](#1-simple--å¯¼èˆª)
  - [2. Spread (åä½œè¦†ç›–)](#2-spread--åä½œè¦†ç›–)
  - [3. Adversary (æ¬ºéª—ä¸æ¨ç†)](#3-adversary--æ¬ºéª—ä¸æ¨ç†)
  - [4. Push (å¯¹æŠ—æ¨æŒ¤)](#4-push--å¯¹æŠ—æ¨æŒ¤)
  - [5. Tag (è¿½é€æ•è·)](#5-tag--è¿½é€æ•è·)
  - [6. Crypto (åŠ å¯†é€šä¿¡)](#6-crypto--åŠ å¯†é€šä¿¡)
  - [7. Reference (å¤šä»»åŠ¡é€šä¿¡)](#7-reference--å¤šä»»åŠ¡é€šä¿¡)
  - [8. Speaker-Listener (å•å‘é€šä¿¡)](#8-speaker-listener--å•å‘é€šä¿¡)
  - [9. World Comm (å¤§è§„æ¨¡åè°ƒ)](#9-world-comm--å¤§è§„æ¨¡åè°ƒ)
- [æç¤ºè¯å·¥ç¨‹](#-æç¤ºè¯å·¥ç¨‹)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [Benchmark æ‰¹é‡è¯„æµ‹](#-benchmark-æ‰¹é‡è¯„æµ‹)
- [å®éªŒç»“æœ](#-å®éªŒç»“æœ)

---

## ğŸŒŸ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº **PettingZoo MPE (Multi-agent Particle Environment)** çš„ LLM å¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚å®ƒå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›´æ¥ä½œä¸ºæ™ºèƒ½ä½“çš„ã€Œå†³ç­–å¤§è„‘ã€ï¼Œåœ¨ 9 ä¸ªç»å…¸çš„å¤šæ™ºèƒ½ä½“åšå¼ˆåœºæ™¯ä¸­è¿›è¡Œé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰æ¨ç†å†³ç­–ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†ä¼ ç»Ÿ RL ä¸­ç”±ç¥ç»ç½‘ç»œç­–ç•¥å®Œæˆçš„ "è§‚æµ‹ â†’ åŠ¨ä½œ" æ˜ å°„ï¼Œæ›¿æ¢ä¸º "è§‚æµ‹ â†’ è‡ªç„¶è¯­è¨€æç¤ºè¯ â†’ LLM æ¨ç† â†’ JSON åŠ¨ä½œè¾“å‡º"ã€‚

<!-- ğŸ–¼ï¸ PLACEHOLDER: æ ¸å¿ƒæµç¨‹å›¾ / Core Pipeline Diagram -->
<!-- ![Core Pipeline](./assets/pipeline.png) -->
<!-- å»ºè®®æ”¾ç½®: ä¸€å¼ å±•ç¤º Observation â†’ Prompt â†’ LLM â†’ Action â†’ Env Step çš„æµç¨‹å›¾ -->
<!-- å°ºå¯¸å»ºè®®: 900x300, SVG æˆ– PNG æ ¼å¼ -->

### ä¸»è¦ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|:---:|:---|
| ğŸ¯ **9 ä¸ªæ¸¸æˆç¯å¢ƒ** | è¦†ç›–åä½œã€å¯¹æŠ—ã€é€šä¿¡ã€æ¬ºéª—ç­‰å¤šç§åšå¼ˆèŒƒå¼ |
| ğŸ¤– **å¤š LLM åç«¯** | æ”¯æŒ DeepSeek / Qwen / GPT / Gemini / Ollama / Transformers / vLLM |
| ğŸ“Š **ç»“æ„åŒ–è¾“å‡º** | æ¯è½®ä¿å­˜ JSON æ—¥å¿—ï¼ˆè§‚æµ‹ã€æ€ç»´é“¾ã€åŠ¨ä½œã€å¥–åŠ±ï¼‰å’Œ MP4 è§†é¢‘ |
| ğŸ§© **æ¨¡å—åŒ–æç¤ºè¯** | æ¯ä¸ªæ¸¸æˆè§£è€¦ä¸º 4 å¤§æ¨¡å—ï¼šä»»åŠ¡ç›®æ ‡ã€ç‰©ç†è§„åˆ™ã€åŠ¨ä½œæ ¼å¼ã€å¯¼èˆªç­–ç•¥ |
| ğŸ”„ **æ‰¹é‡è¯„æµ‹** | `benchmark_runner.py` æ”¯æŒå¤šè½®æ¬¡ (N episodes) + å¤šç§å­ + è·¨ç¯å¢ƒæ±‡æ€»ç»Ÿè®¡ |

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
MPE_muiltiagent_benchmark/
â”‚
â”œâ”€â”€ ğŸ“‚ prompt/                      # æç¤ºè¯æ¨¡å— (æ¯ä¸ªæ¸¸æˆä¸€ä¸ªæ–‡ä»¶)
â”‚   â”œâ”€â”€ prompt_for_simple.py        #   â†’ 4 ä¸ªå‡½æ•°: task, physics, action, hints
â”‚   â”œâ”€â”€ prompt_for_spread.py
â”‚   â”œâ”€â”€ prompt_for_adv.py
â”‚   â”œâ”€â”€ prompt_for_push.py
â”‚   â”œâ”€â”€ prompt_for_tag.py
â”‚   â”œâ”€â”€ prompt_for_crypto.py
â”‚   â”œâ”€â”€ prompt_for_reference.py
â”‚   â”œâ”€â”€ prompt_for_speaker_listener.py
â”‚   â””â”€â”€ prompt_for_world_comm.py
â”‚
â”œâ”€â”€ ğŸ“‚ obs/                         # è§‚æµ‹è§£æå™¨ (raw â†’ structured dict)
â”‚   â”œâ”€â”€ parse_simple_obs.py
â”‚   â”œâ”€â”€ parse_spread_obs.py
â”‚   â”œâ”€â”€ parse_adv_obs.py
â”‚   â”œâ”€â”€ parse_push_obs.py
â”‚   â”œâ”€â”€ parse_tag_obs.py
â”‚   â”œâ”€â”€ parse_crypto_obs.py
â”‚   â”œâ”€â”€ parse_reference_obs.py
â”‚   â”œâ”€â”€ parse_speaker_listener_obs.py
â”‚   â””â”€â”€ parse_world_comm_obs.py
â”‚
â”œâ”€â”€ ğŸ“„ utils_api.py                 # ç»Ÿä¸€æ¨ç†å¼•æ“ (APIInferencer + get_api_engine)
â”œâ”€â”€ ğŸ“„ benchmark_runner.py          # æ‰¹é‡è¯„æµ‹è„šæœ¬ (N episodes Ã— 9 games)
â”‚
â”œâ”€â”€ ğŸ® simple.py                    # æ¸¸æˆä¸»è„šæœ¬ Ã—9
â”œâ”€â”€ ğŸ® spread_API.py
â”œâ”€â”€ ğŸ® adv_API.py
â”œâ”€â”€ ğŸ® push.py
â”œâ”€â”€ ğŸ® tag_API.py
â”œâ”€â”€ ğŸ® crypto.py
â”œâ”€â”€ ğŸ® reference.py
â”œâ”€â”€ ğŸ® speaker_listener.py
â””â”€â”€ ğŸ® world_comm.py
```

### å•æ­¥æ‰§è¡Œæµç¨‹

```mermaid
graph LR
    A["ğŸŒ PettingZoo Env<br/>raw obs (numpy)"] --> B["ğŸ“ Obs Parser<br/>parse_xxx_obs()"]
    B --> C["ğŸ“ Prompt Builder<br/>user_prompt_xxx()"]
    C --> D["ğŸ¤– LLM Engine<br/>APIInferencer"]
    D --> E["ğŸ¯ JSON Action<br/>[a0, a1, a2, a3, a4]"]
    E --> F["ğŸŒ env.step(actions)"]
    F --> G["ğŸ“Š Reward + Log"]
    G --> A

    style A fill:#e1f5fe
    style D fill:#fff3e0
    style G fill:#e8f5e9
```

æ¯ä¸ªæ¸¸æˆè„šæœ¬éƒ½éµå¾ªç›¸åŒçš„ä¸»å¾ªç¯æ¨¡å¼ï¼š

```python
for step in range(MAX_STEPS):
    for agent_id in env.agents:
        obs_struct = parse_xxx_obs(observations[agent_id])  # â‘  è§£æè§‚æµ‹
        full_prompt = user_prompt_xxx(agent_id, step, obs_struct)  # â‘¡ ç»„è£…æç¤ºè¯
        action_vec, thought = llm_engine.generate_action(sys_prompt, full_prompt)  # â‘¢ LLM æ¨ç†
        actions[agent_id] = np.clip(action_vec, 0.0, 1.0)  # â‘£ è£å‰ªåŠ¨ä½œ
    observations, rewards, _, _, _ = env.step(actions)  # â‘¤ ç¯å¢ƒæ­¥è¿›
```

---

## ğŸ® æ¸¸æˆç¯å¢ƒæ€»è§ˆ

<!-- ğŸ–¼ï¸ PLACEHOLDER: æ¸¸æˆæˆªå›¾åˆé›† / Game Screenshots Collage -->
<!-- ![All Games](./assets/all_games_collage.png) -->
<!-- å»ºè®®æ”¾ç½®: 3Ã—3 ä¹å®«æ ¼, æ¯æ ¼ä¸€ä¸ªæ¸¸æˆçš„æˆªå›¾, å¸¦æ¸¸æˆåæ ‡æ³¨ -->
<!-- å°ºå¯¸å»ºè®®: 1200x1200 -->

| # | æ¸¸æˆ | ç±»å‹ | æ™ºèƒ½ä½“ | æ ¸å¿ƒæŒ‘æˆ˜ | é€šä¿¡ |
|:-:|:-----|:-----|:------:|:---------|:----:|
| 1 | **Simple** | å¯¼èˆª | 1 | å•æ™ºèƒ½ä½“å¯¼èˆªåˆ°åœ°æ ‡ | âœ— |
| 2 | **Spread** | åä½œ | N (é»˜è®¤ 3) | å¤šæ™ºèƒ½ä½“è¦†ç›–å¤šåœ°æ ‡ + é¿ç¢° | âœ— |
| 3 | **Adversary** | å¯¹æŠ— | 1 Adv + N Good | æ¬ºéª—ï¼šGood è¿·æƒ‘ Advï¼ŒAdv æ¨ç†ç›®æ ‡ | âœ— |
| 4 | **Push** | å¯¹æŠ— | 1 Adv + 1 Good | ç‰©ç†æ¨æŒ¤é˜»æˆª vs å†²åˆºåˆ°è¾¾ç›®æ ‡ | âœ— |
| 5 | **Tag** | è¿½é€ | 3 Pred + 1 Prey | åˆä½œè¿½æ• vs é€ƒè·‘ | âœ— |
| 6 | **Crypto** | é€šä¿¡ | Alice + Bob + Eve | ä¿¡æ¯åŠ å¯†ä¼ è¾“ vs çªƒå¬ç ´è¯‘ | âœ“ |
| 7 | **Reference** | é€šä¿¡ | 2 (äº’ä¸ºè¯´/å¬è€…) | è§‚å¯Ÿä¼™ä¼´ç›®æ ‡ â†’ å¹¿æ’­ä¿¡å· â†’ å¬ä¿¡å·å¯¼èˆª | âœ“ |
| 8 | **Speaker-Listener** | é€šä¿¡ | 1 Speaker + 1 Listener | å•å‘é€šä¿¡ï¼šè¯´è€…ä¼ ç›®æ ‡ï¼Œå¬è€…å¯¼èˆª | âœ“ |
| 9 | **World Comm** | å¤§è§„æ¨¡ | 4 Adv + 2 Prey | Leader å¹¿æ’­çŒç‰©åæ ‡ï¼ŒHunter ååŒè¿½æ• | âœ“ |

---

## ğŸ“‹ å„æ¸¸æˆè¯¦è§£

---

### 1. Simple | å¯¼èˆª

<!-- ğŸ¬ PLACEHOLDER: Simple æ¸¸æˆæ¼”ç¤ºè§†é¢‘ / Simple Game Demo Video -->
<!-- ![Simple Demo](./assets/videos/simple_demo.mp4) -->
<!-- æˆ–è€…ä½¿ç”¨ GIF: ![Simple Demo](./assets/gifs/simple_demo.gif) -->
<!-- æ”¾ç½®ä½ç½®: æ”¾åœ¨æœ¬èŠ‚æ ‡é¢˜ä¸‹æ–¹, å°ºå¯¸å»ºè®® 400x400 -->

**ç¯å¢ƒåç§°**: `simple_v3` | **PettingZoo æ¨¡å—**: `pettingzoo.mpe.simple_v3`

#### ğŸ¯ æ¸¸æˆç›®æ ‡

å•ä¸ªæ™ºèƒ½ä½“ç§»åŠ¨è‡³å•ä¸ªåœ°æ ‡ä½ç½®ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| å­—æ®µ | ç»´åº¦ | å«ä¹‰ |
|:-----|:----:|:-----|
| `vel` | 2 | è‡ªèº«é€Ÿåº¦ `[vx, vy]` |
| `landmark_rel` | 3 | åœ°æ ‡ç›¸å¯¹ä½ç½® `[dx, dy, distance]` |

> **è§‚æµ‹è¯­ä¹‰**: `obs = [vel_x, vel_y, dx, dy]`ï¼Œå…¶ä¸­ `(dx, dy) = landmark_pos - agent_pos`ã€‚

#### ğŸ’° å¥–åŠ±å‡½æ•°

```
reward = -â€–agent_pos - landmark_posâ€–Â² = -(dxÂ² + dyÂ²)
```

è·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼ˆæœ€å¤§ä¸º 0ï¼‰ã€‚

#### ğŸ•¹ï¸ åŠ¨ä½œç©ºé—´

| ç´¢å¼• | å«ä¹‰ | å–å€¼èŒƒå›´ |
|:----:|:-----|:-------:|
| `a[0]` | æ— æ“ä½œ (No-Op) | [0, 1] |
| `a[1]` | å·¦æ¨åŠ› (Left) | [0, 1] |
| `a[2]` | å³æ¨åŠ› (Right) | [0, 1] |
| `a[3]` | ä¸‹æ¨åŠ› (Down) | [0, 1] |
| `a[4]` | ä¸Šæ¨åŠ› (Up) | [0, 1] |

**å‡€åŠ›**: `F_x = (a[2] - a[1]) Ã— sensitivity`, `F_y = (a[4] - a[3]) Ã— sensitivity`

<!-- ğŸ“Š PLACEHOLDER: Simple å®éªŒç»“æœè¡¨æ ¼ / Simple Experiment Results Table -->
<!-- å»ºè®®å†…å®¹: ä¸åŒ LLM çš„å¹³å‡å¥–åŠ±å¯¹æ¯”è¡¨ -->
<!--
| Model | Mean Reward | Std Dev | Episodes |
|:------|:----------:|:-------:|:--------:|
| Qwen-3-Max | -0.xxx | Â±0.xxx | 5 |
| DeepSeek-Chat | -0.xxx | Â±0.xxx | 5 |
| GPT-4o | -0.xxx | Â±0.xxx | 5 |
-->

---

### 2. Spread | åä½œè¦†ç›–

<!-- ğŸ¬ PLACEHOLDER: Spread æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Spread Demo](./assets/videos/spread_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_spread_v3` | **æ™ºèƒ½ä½“æ•°**: N (é»˜è®¤ 3)

#### ğŸ¯ æ¸¸æˆç›®æ ‡

N ä¸ªæ™ºèƒ½ä½“åä½œè¦†ç›– N ä¸ªåœ°æ ‡ï¼ŒåŒæ—¶é¿å…ç¢°æ’ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| å­—æ®µ | ç»´åº¦ | å«ä¹‰ |
|:-----|:----:|:-----|
| `self_vel` | 2 | è‡ªèº«é€Ÿåº¦ |
| `self_pos` | 2 | è‡ªèº«ä½ç½® |
| `landmark_rel` | 2Ã—N | å„åœ°æ ‡ç›¸å¯¹ä½ç½® `(landmark - self)` |
| `other_agent_rel` | 2Ã—(N-1) | å…¶ä»–æ™ºèƒ½ä½“ç›¸å¯¹ä½ç½® |
| `comm` | 2Ã—(N-1) | å…¶ä»–æ™ºèƒ½ä½“é€šä¿¡ä¿¡å· (é€šå¸¸ä¸º 0) |

#### ğŸ’° å¥–åŠ±å‡½æ•°

```
global_reward = -Î£_landmark min_agent â€–agent - landmarkâ€–
local_reward  = -1.0  (æ¯æ¬¡ç¢°æ’)
total_reward  = global Ã— (1 - local_ratio) + local Ã— local_ratio
```

å…¶ä¸­ **ç¢°æ’åˆ¤å®š**: `dist(agent_i, agent_j) < size_i + size_j` (size = 0.15)ã€‚

> `local_ratio` é»˜è®¤ä¸º 0.5ï¼Œç”¨äºå¹³è¡¡å…¨å±€è¦†ç›–å’Œå±€éƒ¨é¿ç¢°ã€‚

#### ğŸ•¹ï¸ åŠ¨ä½œç©ºé—´

ä¸ Simple ç›¸åŒçš„ 5 ç»´è¿ç»­åŠ›å‘é‡ `[no_op, left, right, down, up]`ã€‚

<!-- ğŸ“Š PLACEHOLDER: Spread å®éªŒç»“æœè¡¨æ ¼ -->

---

### 3. Adversary | æ¬ºéª—ä¸æ¨ç†

<!-- ğŸ¬ PLACEHOLDER: Adversary æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Adversary Demo](./assets/videos/adversary_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_adversary_v3` | **æ™ºèƒ½ä½“**: 1 Adversary + N Good Agents (é»˜è®¤ 3)

#### ğŸ¯ æ¸¸æˆç›®æ ‡

- **Good Agents (ç»¿è‰²)**: çŸ¥é“ç›®æ ‡åœ°æ ‡ï¼Œéœ€è¦å é¢†ç›®æ ‡ä¸”**æ¬ºéª—** Adversary å»é”™è¯¯çš„åœ°æ ‡ã€‚ç­–ç•¥ï¼šåˆ†å…µ â€” ä¸€äººå†²ç›®æ ‡ï¼Œä¸€äººåšè¯±é¥µã€‚
- **Adversary (çº¢è‰²)**: ä¸çŸ¥é“å“ªä¸ªæ˜¯ç›®æ ‡ï¼Œéœ€è¦é€šè¿‡**è§‚å¯Ÿ** Good Agents çš„è¡Œä¸ºæ¨ç†å‡ºç›®æ ‡ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

**Good Agent è§‚æµ‹**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `vel` | è‡ªèº«é€Ÿåº¦ |
| `goal` | ç›®æ ‡åœ°æ ‡ç›¸å¯¹ä½ç½® + è·ç¦» |
| `landmarks[]` | æ‰€æœ‰åœ°æ ‡ç›¸å¯¹ä½ç½® |
| `adversary` | å¯¹æ‰‹ç›¸å¯¹ä½ç½® + è·ç¦» |
| `teammates[]` | é˜Ÿå‹ç›¸å¯¹ä½ç½® |

**Adversary è§‚æµ‹**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `vel` | è‡ªèº«é€Ÿåº¦ |
| `landmarks[]` | æ‰€æœ‰åœ°æ ‡ç›¸å¯¹ä½ç½® (ä¸çŸ¥é“å“ªä¸ªæ˜¯ç›®æ ‡) |
| `good_agents[]` | æ‰€æœ‰ Good Agent ç›¸å¯¹ä½ç½® |

#### ğŸ’° å¥–åŠ±å‡½æ•°

**é›¶å’Œåšå¼ˆ**:
- Good Agents: å¸Œæœ› Adversary **è¿œç¦»**çœŸå®ç›®æ ‡
- Adversary: å¸Œæœ›è‡ªå·±**é è¿‘**çœŸå®ç›®æ ‡

<!-- ğŸ“Š PLACEHOLDER: Adversary å®éªŒç»“æœè¡¨æ ¼ -->

---

### 4. Push | å¯¹æŠ—æ¨æŒ¤

<!-- ğŸ¬ PLACEHOLDER: Push æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Push Demo](./assets/videos/push_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_push_v3` | **æ™ºèƒ½ä½“**: 1 Adversary + 1 Good Agent

#### ğŸ¯ æ¸¸æˆç›®æ ‡

- **Good Agent (Runner)**: å†²åˆ°**çœŸå®ç›®æ ‡**åœ°æ ‡ï¼ˆæœ‰ä¸€ä¸ªå‡åœ°æ ‡å¹²æ‰°ï¼‰ã€‚
- **Adversary (Blocker)**: é˜»æ­¢ Good Agent åˆ°è¾¾ç›®æ ‡ã€‚Adversary çœ‹åˆ°ä¸¤ä¸ªåœ°æ ‡ä½†ä¸çŸ¥é“å“ªä¸ªæ˜¯çœŸçš„ï¼Œéœ€è¦ä» Good Agent çš„è¿åŠ¨ä¸­æ¨æ–­ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

**Good Agent è§‚æµ‹**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `vel`, `speed` | è‡ªèº«è¿åŠ¨çŠ¶æ€ |
| `goal_rel`, `goal_dist` | çœŸå®ç›®æ ‡ç›¸å¯¹ä½ç½®åŠè·ç¦» |
| `fake_rel`, `fake_dist` | å‡ç›®æ ‡ç›¸å¯¹ä½ç½®åŠè·ç¦» |
| `opponent_rel`, `opponent_dist` | å¯¹æ‰‹ç›¸å¯¹ä½ç½®åŠè·ç¦» |

**Adversary è§‚æµ‹**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `vel`, `speed` | è‡ªèº«è¿åŠ¨çŠ¶æ€ |
| `landmarks[]` | ä¸¤ä¸ªåœ°æ ‡çš„ç›¸å¯¹ä½ç½®ï¼ˆä¸çŸ¥é“çœŸå‡ï¼‰ |
| `opponent_rel`, `opponent_dist` | Good Agent ç›¸å¯¹ä½ç½® |

#### ğŸ’° å¥–åŠ±å‡½æ•°

- **Good Agent**: æ¥è¿‘çœŸå®ç›®æ ‡æ—¶å¥–åŠ±å¢åŠ 
- **Adversary**: Good Agent è¿œç¦»ç›®æ ‡æ—¶å¥–åŠ±å¢åŠ 
- **ç‰©ç†ç‰¹æ€§**: Adversary è´¨é‡æ›´å¤§ï¼Œç¢°æ’æ—¶å¯ä»¥æ¨å¼€ Good Agent

<!-- ğŸ“Š PLACEHOLDER: Push å®éªŒç»“æœè¡¨æ ¼ -->

---

### 5. Tag | è¿½é€æ•è·

<!-- ğŸ¬ PLACEHOLDER: Tag æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Tag Demo](./assets/videos/tag_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_tag_v3` | **æ™ºèƒ½ä½“**: 3 Predator + 1 Prey + 2 Obstacles

#### ğŸ¯ æ¸¸æˆç›®æ ‡

- **Predator (ğŸ”´ ç‹¼)**: åˆä½œè¿½æ•çŒç‰©ï¼Œç‰©ç†ç¢°æ’å³æˆåŠŸã€‚
- **Prey (ğŸŸ¢ ç¾Š)**: åˆ©ç”¨éšœç¢ç‰©é€ƒè·‘ï¼Œä¿æŒåœ¨è¾¹ç•Œå†…ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| å­—æ®µ | Predator | Prey |
|:-----|:--------:|:----:|
| `self_vel`, `self_pos` | âœ“ | âœ“ |
| `obstacles_rel[]` | âœ“ (éšœç¢ç‰©) | âœ“ (éšœç¢ç‰©) |
| `enemies[]` | âœ“ (çŒç‰©ä½ç½®) | âœ“ (æ•é£Ÿè€…ä½ç½®) |
| `teammates[]` | âœ“ (å…¶ä»–æ•é£Ÿè€…) | âœ— |

#### ğŸ’° å¥–åŠ±å‡½æ•°

| è§’è‰² | æ¡ä»¶ | å¥–åŠ± |
|:----:|:-----|:----:|
| Predator | ç¢°æ’çŒç‰© | **+10.0** |
| Predator | æ¯æ­¥è·ç¦»æƒ©ç½š | **-0.1** Ã— dist |
| Prey | è¢«æ•è· | **-10.0** |
| Prey | å‡ºç•Œ | **-1.0** / step |
| Prey | å®‰å…¨å­˜æ´» | **+0.1** / step |

> **å…³é”®ä¸å¯¹ç§°**: çŒç‰©çš„åŠ é€Ÿåº¦æ¯”æ•é£Ÿè€…é«˜ï¼Œå› æ­¤æ•é£Ÿè€…å¿…é¡»ååŒåŒ…å›´ã€‚

<!-- ğŸ“Š PLACEHOLDER: Tag å®éªŒç»“æœè¡¨æ ¼ -->

---

### 6. Crypto | åŠ å¯†é€šä¿¡

<!-- ğŸ¬ PLACEHOLDER: Crypto æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Crypto Demo](./assets/videos/crypto_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_crypto_v3` | **æ™ºèƒ½ä½“**: Alice + Bob + Eve

#### ğŸ¯ æ¸¸æˆç›®æ ‡

- **Alice (åŠ å¯†è€…)**: å°†ç§˜å¯†æ¶ˆæ¯ M ä¸å¯†é’¥ K æ··åˆï¼Œè¾“å‡ºå¯†æ–‡ C ç»™ Bobã€‚
- **Bob (è§£å¯†è€…)**: ç”¨å…±äº«å¯†é’¥ K è¿˜åŸå‡ºåŸå§‹æ¶ˆæ¯ Mã€‚
- **Eve (çªƒå¬è€…)**: ä»…å‡­å¯†æ–‡ C çŒœæµ‹æ¶ˆæ¯ Mï¼ˆæ²¡æœ‰å¯†é’¥ï¼‰ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| è§’è‰² | è§‚æµ‹å†…å®¹ |
|:----:|:---------|
| Alice | æ¶ˆæ¯ M (4ç»´) + å¯†é’¥ K (4ç»´) |
| Bob | å¯†é’¥ K (4ç»´) + å¯†æ–‡ C (ä¸Šä¸€æ­¥ Alice çš„è¾“å‡º) |
| Eve | å¯†æ–‡ C (ä¸Šä¸€æ­¥ Alice çš„è¾“å‡º) |

#### ğŸ’° å¥–åŠ±å‡½æ•°

- **Alice & Bob**: Bob æ­£ç¡®è¿˜åŸæ¶ˆæ¯æ—¶å¥–åŠ±é«˜ï¼ŒEve çŒœå¯¹æ—¶å¥–åŠ±ä½
- **Eve**: çŒœæµ‹è¶Šæ¥è¿‘çœŸå®æ¶ˆæ¯ï¼Œå¥–åŠ±è¶Šé«˜

**åŠ¨ä½œç©ºé—´**: 4 ç»´è¿ç»­å‘é‡ `[v1, v2, v3, v4]` âˆˆ [0, 1]

> **æ³¨æ„**: MPE ä¸­ Bob çœ‹åˆ°çš„å¯†æ–‡æ˜¯ Alice **ä¸Šä¸€æ­¥**å‘å‡ºçš„ï¼ˆå»¶è¿Ÿä¸€å¸§ï¼‰ã€‚

<!-- ğŸ“Š PLACEHOLDER: Crypto å®éªŒç»“æœè¡¨æ ¼ -->

---

### 7. Reference | å¤šä»»åŠ¡é€šä¿¡

<!-- ğŸ¬ PLACEHOLDER: Reference æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Reference Demo](./assets/videos/reference_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_reference_v3` | **æ™ºèƒ½ä½“**: 2 ä¸ª (äº’ä¸ºè¯´è€…/å¬è€…)

#### ğŸ¯ æ¸¸æˆç›®æ ‡

æ¯ä¸ªæ™ºèƒ½ä½“æ—¢æ˜¯è¯´è€…åˆæ˜¯å¬è€…ã€‚æ¯ä¸ªæ™ºèƒ½ä½“è§‚å¯Ÿåˆ°**å¯¹æ–¹çš„ç›®æ ‡åœ°æ ‡é¢œè‰²**ï¼Œéœ€è¦é€šè¿‡é€šä¿¡å¸®åŠ©å¯¹æ–¹å¯¼èˆªã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| å­—æ®µ | ç»´åº¦ | å«ä¹‰ |
|:-----|:----:|:-----|
| `vel` | 2 | è‡ªèº«é€Ÿåº¦ |
| `landmarks` | 3Ã—2 | 3 ä¸ªåœ°æ ‡ç›¸å¯¹ä½ç½® |
| `partner_goal_rgb` | 3 | å¯¹æ–¹ç›®æ ‡çš„ RGB é¢œè‰² |
| `partner_target_id` | 1 | æ¨æ–­å‡ºçš„å¯¹æ–¹ç›®æ ‡ ID |
| `heard_signal` | - | æ¥æ”¶åˆ°çš„é€šä¿¡ä¿¡å· |

**åŠ¨ä½œç©ºé—´**: 15 ç»´ = 5 (è¿åŠ¨) + 10 (é€šä¿¡ä¿¡å·)

<!-- ğŸ“Š PLACEHOLDER: Reference å®éªŒç»“æœè¡¨æ ¼ -->

---

### 8. Speaker-Listener | å•å‘é€šä¿¡

<!-- ğŸ¬ PLACEHOLDER: Speaker-Listener æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![Speaker-Listener Demo](./assets/videos/speaker_listener_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_speaker_listener_v4` | **æ™ºèƒ½ä½“**: 1 Speaker + 1 Listener

#### ğŸ¯ æ¸¸æˆç›®æ ‡

- **Speaker**: çœ‹åˆ°ç›®æ ‡åœ°æ ‡ (one-hot å‘é‡)ï¼Œéœ€è¦é€šè¿‡ 3 ç»´é€šä¿¡ä¿¡å·å‘Šè¯‰ Listenerã€‚
- **Listener**: çœ‹ä¸åˆ°ç›®æ ‡ï¼Œä½†èƒ½çœ‹åˆ°åœ°æ ‡ä½ç½®ã€‚æ ¹æ® Speaker ä¼ æ¥çš„ä¿¡å·å¯¼èˆªåˆ°ç›®æ ‡ã€‚

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

**Speaker**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `goal_vector` | ç›®æ ‡åœ°æ ‡ one-hot (3ç»´) |
| `target_landmark_id` | ç›®æ ‡ ID |

**Listener**:

| å­—æ®µ | å«ä¹‰ |
|:-----|:-----|
| `vel` | è‡ªèº«é€Ÿåº¦ |
| `landmarks[]` | 3 ä¸ªåœ°æ ‡ç›¸å¯¹ä½ç½® |
| `comm_vector` | æ¥æ”¶çš„ 3 ç»´é€šä¿¡ä¿¡å· |
| `heard_id` | æ¨æ–­çš„ç›®æ ‡ ID |

**Speaker åŠ¨ä½œ**: 3 ç»´ (é€šä¿¡ä¿¡å·) | **Listener åŠ¨ä½œ**: 5 ç»´ (è¿åŠ¨)

<!-- ğŸ“Š PLACEHOLDER: Speaker-Listener å®éªŒç»“æœè¡¨æ ¼ -->

---

### 9. World Comm | å¤§è§„æ¨¡åè°ƒ

<!-- ğŸ¬ PLACEHOLDER: World Comm æ¸¸æˆæ¼”ç¤ºè§†é¢‘ -->
<!-- ![World Comm Demo](./assets/videos/world_comm_demo.mp4) -->

**ç¯å¢ƒåç§°**: `simple_world_comm_v3` | **æ™ºèƒ½ä½“**: 4 Adversary (1 Leader + 3 Hunter) + 2 Prey

#### ğŸ¯ æ¸¸æˆç›®æ ‡

æœ€å¤æ‚çš„ç¯å¢ƒã€‚åŒ…å«é£Ÿç‰©ã€æ£®æ—ã€éšœç¢ç‰©ã€‚

| è§’è‰² | ç›®æ ‡ |
|:----:|:-----|
| **Leader** | å…¨å±€æ„ŸçŸ¥çŒç‰©ä½ç½®å¹¶å¹¿æ’­åæ ‡ç»™ Hunter |
| **Hunter** | æ ¹æ® Leader çš„ä¿¡å·æˆ–è‡ªèº«è§‚æµ‹è¿½æ•çŒç‰© |
| **Prey** | é€ƒé¿æ‰€æœ‰ Hunterï¼Œå¯»æ‰¾é£Ÿç‰© |

#### ğŸ‘ï¸ è§‚æµ‹ç©ºé—´

| å­—æ®µ | Leader | Hunter | Prey |
|:-----|:------:|:------:|:----:|
| `position`, `velocity` | âœ“ | âœ“ | âœ“ |
| `enemies` (çŒç‰©/å¨èƒ) | âœ“ | âœ“ | âœ“ |
| `teammates` | âœ“ (hunters) | âœ— | âœ“ (partner) |
| `communication` | âœ— (å‘é€è€…) | âœ“ (æ¥æ”¶) | âœ— |
| `landmarks` | âœ“ | âœ“ | âœ“ |

**Leader åŠ¨ä½œ**: 9 ç»´ = 5 (è¿åŠ¨) + 4 (é€šä¿¡: Prey0_X, Prey0_Y, Prey1_X, Prey1_Y)
**Hunter/Prey åŠ¨ä½œ**: 5 ç»´ (è¿åŠ¨)

<!-- ğŸ“Š PLACEHOLDER: World Comm å®éªŒç»“æœè¡¨æ ¼ -->

---

## ğŸ§  æç¤ºè¯å·¥ç¨‹

æ¯ä¸ªæ¸¸æˆçš„æç¤ºè¯è¢«è§£è€¦ä¸º **4 ä¸ªæ ‡å‡†åŒ–æ¨¡å—**ï¼Œå­˜æ”¾åœ¨ `prompt/prompt_for_xxx.py` ä¸­ï¼š

```mermaid
graph TB
    A["get_task_and_reward()"] --> E["å®Œæ•´æç¤ºè¯<br/>Full Prompt"]
    B["get_physics_rules()"] --> E
    C["get_action_and_response_format()"] --> E
    D["get_navigation_hints()"] --> E
    E --> F["LLM"]

    style A fill:#ffcdd2
    style B fill:#c8e6c9
    style C fill:#bbdefb
    style D fill:#fff9c4
```

| æ¨¡å— | å‡½æ•°å | å†…å®¹è¯´æ˜ |
|:-----|:------|:---------|
| **ä»»åŠ¡ä¸å¥–åŠ±** | `get_task_and_reward()` | æ¸¸æˆè§„åˆ™ã€è§’è‰²ç›®æ ‡ã€å¥–åŠ±å…¬å¼ |
| **ç‰©ç†è§„åˆ™** | `get_physics_rules()` | dt, é˜»å°¼, è´¨é‡, ç¢°æ’åˆ¤å®šç­‰ |
| **åŠ¨ä½œæ ¼å¼** | `get_action_and_response_format()` | åŠ¨ä½œç»´åº¦ã€JSON è¾“å‡ºæ ¼å¼ã€few-shot ç¤ºä¾‹ |
| **å¯¼èˆªç­–ç•¥** | `get_navigation_hints()` | åæ ‡ç†è§£ã€è¾¹ç•Œå¤„ç†ã€è§’è‰²ç­–ç•¥ |

### æç¤ºè¯è¾“å‡ºæ ¼å¼ (æ‰€æœ‰æ¸¸æˆç»Ÿä¸€)

```json
{"action": [a0, a1, a2, a3, a4], "notes": "Short Strategy"}
```

### ç‰©ç†å¼•æ“æ ¸å¿ƒå‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ |
|:-----|:--:|:-----|
| æ—¶é—´æ­¥ `dt` | 0.1 | æ¯æ­¥çš„ç‰©ç†ä»¿çœŸæ—¶é—´ |
| é˜»å°¼ `damping` | 0.25 | é€Ÿåº¦è¡°å‡:`v â† 0.75 Ã— v` |
| è´¨é‡ `mass` | 1.0 | é»˜è®¤è´¨é‡ |
| çµæ•åº¦ `sensitivity` | 5.0 | åŠ¨ä½œåˆ°åŠ›çš„æ˜ å°„ç³»æ•° |
| åœ°å›¾èŒƒå›´ | [-1, 1]Â² | Xã€Y è½´èŒƒå›´ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ç¯å¢ƒ

```bash
pip install -r requirements.txt
# æ ¸å¿ƒä¾èµ–: pettingzoo[mpe], openai, numpy, imageio
```

### 2. é…ç½® API Key

**æ–¹å¼ A**: ç¯å¢ƒå˜é‡

```bash
# Linux/Mac
export DEEPSEEK_API_KEY="sk-your-key"
export QWEN_API_KEY="sk-your-key"

# Windows PowerShell
$env:DEEPSEEK_API_KEY = "sk-your-key"
$env:QWEN_API_KEY = "sk-your-key"
```

**æ–¹å¼ B**: åˆ›å»º `.env` æ–‡ä»¶ (æ¨è)

```env
DEEPSEEK_API_KEY=sk-your-key
QWEN_API_KEY=sk-your-key
OPENAI_API_KEY=sk-your-key
```

**æ–¹å¼ C**: è¿è¡Œæ—¶ä¼ å‚

```python
run_spread_game("deepseek", "demo.mp4", api_key="sk-your-key", base_url="https://api.deepseek.com")
```

### 3. è¿è¡Œå•ä¸ªæ¸¸æˆ

```python
# simple.py - æœ€ç®€å•çš„å…¥é—¨æ¡ˆä¾‹
python simple.py

# spread_API.py - åä½œè¦†ç›–
python spread_API.py

# tag_API.py - è¿½é€æ•è·
python tag_API.py
```

### 4. è¾“å‡ºæ–‡ä»¶

æ¯æ¬¡è¿è¡Œä¼šåœ¨å½“å‰ç›®å½•ç”Ÿæˆï¼š

| æ–‡ä»¶ | è¯´æ˜ |
|:-----|:-----|
| `xxx_demo.mp4` | æ¸¸æˆå½•åƒè§†é¢‘ |
| `xxx_demo.json` | è¯¦ç»†æ—¥å¿— (æ¯æ­¥: obs, action, thought, reward) |

---

## ğŸ“Š Benchmark æ‰¹é‡è¯„æµ‹

ä½¿ç”¨ `benchmark_runner.py` è¿›è¡Œæ ‡å‡†åŒ–è¯„æµ‹ï¼š

```python
from benchmark_runner import run_benchmark

# å•ä¸ªç¯å¢ƒè¯„æµ‹ (5 ä¸ª episode, æ¯ä¸ªä½¿ç”¨ä¸åŒç§å­)
result = run_benchmark(
    env_name="spread",
    provider="qwen",
    episodes=5,
    output_dir="results/benchmarks",
)

print(f"Mean Reward: {result['mean_reward']:.4f} Â± {result['std_reward']:.4f}")
```

### å…¨ç¯å¢ƒæ‰¹é‡è¯„æµ‹

```python
environments = [
    "simple", "spread", "adversary", "push", "tag",
    "crypto", "reference", "speaker_listener", "world_comm"
]

for env_name in environments:
    result = run_benchmark(env_name=env_name, provider="qwen", episodes=5)
```

### è¯„æµ‹è¾“å‡ºç»“æ„

```
results/benchmarks/
â”œâ”€â”€ spread/
â”‚   â”œâ”€â”€ spread_ep1.mp4          # Episode 1 å½•åƒ
â”‚   â”œâ”€â”€ spread_ep1.json         # Episode 1 æ—¥å¿—
â”‚   â”œâ”€â”€ spread_ep2.mp4
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tag/
â”‚   â”œâ”€â”€ tag_ep1.mp4
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

---

## ğŸ“ˆ å®éªŒç»“æœ

<!-- ğŸ“Š PLACEHOLDER: å®éªŒç»“æœæ±‡æ€»è¡¨ / Experiment Results Summary Table -->
<!-- å»ºè®®æ ¼å¼: -->
<!--
### å„æ¨¡å‹åœ¨ 9 ä¸ªç¯å¢ƒä¸­çš„å¹³å‡å¥–åŠ±

| Environment | Qwen-3-Max | DeepSeek-Chat | GPT-4o | Gemini-1.5-Pro |
|:------------|:----------:|:-------------:|:------:|:--------------:|
| Simple | | | | |
| Spread | | | | |
| Adversary | | | | |
| Push | | | | |
| Tag | | | | |
| Crypto | | | | |
| Reference | | | | |
| Speaker-Listener | | | | |
| World Comm | | | | |
-->

<!-- ğŸ“Š PLACEHOLDER: å¥–åŠ±æ›²çº¿å›¾ / Reward Curves Chart -->
<!-- ![Reward Curves](./assets/charts/reward_curves.png) -->
<!-- å»ºè®®: æŠ˜çº¿å›¾, Xè½´=Episode, Yè½´=Mean Reward, ä¸åŒé¢œè‰²=ä¸åŒæ¨¡å‹ -->

<!-- ğŸ“Š PLACEHOLDER: é›·è¾¾å›¾ / Radar Chart -->
<!-- ![Radar Chart](./assets/charts/radar_chart.png) -->
<!-- å»ºè®®: é›·è¾¾å›¾, 9ä¸ªè½´=9ä¸ªæ¸¸æˆçš„å½’ä¸€åŒ–å¾—åˆ†, ä¸åŒé¢œè‰²=ä¸åŒæ¨¡å‹ -->

<!-- ğŸ¬ PLACEHOLDER: ç²¾å½©å¯¹å±€é›†é”¦ / Highlight Replay Videos -->
<!-- å»ºè®®æ”¾ç½®å¤šä¸ªè§†é¢‘, å±•ç¤ºå…³é”®åœºæ™¯: -->
<!-- 1. Tag ä¸­çŒäººæˆåŠŸåŒ…å›´çŒç‰© -->
<!-- 2. Adversary ä¸­ Good Agent æˆåŠŸæ¬ºéª— -->
<!-- 3. Crypto ä¸­ Bob æˆåŠŸè§£å¯† -->

> **ğŸ’¡ å¦‚ä½•æ”¾ç½®å®éªŒç»“æœ**: å‚è§ä¸‹æ–¹ [å ä½ç¬¦ä½¿ç”¨æŒ‡å—](#-å ä½ç¬¦ä½¿ç”¨æŒ‡å—)ã€‚

---

## ğŸ“Œ å ä½ç¬¦ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£ä¸­é¢„ç•™äº†ä»¥ä¸‹ç±»å‹çš„å ä½ç¬¦ï¼Œæ–¹ä¾¿æ‚¨åç»­æ·»åŠ å®éªŒç´ æï¼š

### ğŸ¬ è§†é¢‘å ä½ç¬¦

æ¯ä¸ªæ¸¸æˆå°èŠ‚æ ‡é¢˜ä¸‹æ–¹éƒ½é¢„ç•™äº†è§†é¢‘ä½ç½®ã€‚æ›¿æ¢æ–¹æ³•ï¼š

1. å°†è§†é¢‘æ–‡ä»¶æ”¾å…¥ `docs/assets/videos/` ç›®å½•
2. å–æ¶ˆæ³¨é‡Šå¯¹åº”è¡Œå¹¶ä¿®æ”¹è·¯å¾„ï¼š

```markdown
<!-- å–æ¶ˆè¿™è¡Œæ³¨é‡Šï¼Œä¿®æ”¹è·¯å¾„ -->
![Simple Demo](./assets/videos/simple_demo.mp4)

<!-- æˆ–ä½¿ç”¨ GIF -->
![Simple Demo](./assets/gifs/simple_demo.gif)
```

> **æ¨èæ ¼å¼**: MP4 (H.264) æˆ– GIFï¼Œå°ºå¯¸ 400Ã—400ï¼Œå¸§ç‡ 5-10 fpsã€‚

### ğŸ“Š è¡¨æ ¼å ä½ç¬¦

æ¯ä¸ªæ¸¸æˆæœ«å°¾é¢„ç•™äº†å®éªŒç»“æœè¡¨æ ¼ä½ç½®ã€‚å¡«å…¥ç¤ºä¾‹ï¼š

```markdown
| Model | Mean Reward | Std Dev | Episodes |
|:------|:----------:|:-------:|:--------:|
| Qwen-3-Max | -12.345 | Â±1.234 | 5 |
| DeepSeek-Chat | -15.678 | Â±2.345 | 5 |
```

### ğŸ–¼ï¸ å›¾ç‰‡å ä½ç¬¦

æ–‡æ¡£ä¸­é¢„ç•™äº†ä»¥ä¸‹å›¾ç‰‡ä½ç½®ï¼š

| ä½ç½® | å»ºè®®å†…å®¹ | å»ºè®®å°ºå¯¸ |
|:-----|:---------|:--------:|
| é¡¶éƒ¨æ¨ªå¹… | ä¹å®«æ ¼æ¸¸æˆæˆªå›¾æ‹¼æ¥ | 1200Ã—400 |
| é¡¹ç›®æ¦‚è¿°ä¸‹æ–¹ | Obsâ†’Promptâ†’LLMâ†’Action æµç¨‹å›¾ | 900Ã—300 |
| æ¸¸æˆæ€»è§ˆä¸Šæ–¹ | 3Ã—3 ä¹å®«æ ¼å„æ¸¸æˆæˆªå›¾ | 1200Ã—1200 |
| å®éªŒç»“æœåŒº | å¥–åŠ±æ›²çº¿å›¾ / é›·è¾¾å›¾ | 800Ã—600 |

### ğŸ”— é“¾æ¥å ä½ç¬¦

åœ¨ GitHub æˆ–ç½‘ç«™ä¸Šå‘å¸ƒæ—¶ï¼Œå¯åœ¨æ–‡æ¡£é¡¶éƒ¨æ·»åŠ ï¼š

```markdown
[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/xxxx.xxxxx)
[![Project Page](https://img.shields.io/badge/Project-Page-blue)](https://your-project-page.com)
```

---

---

<a name="english-version"></a>

# ğŸ“– English Documentation

## Table of Contents

- [Project Overview](#-project-overview)
- [System Architecture](#-system-architecture-1)
- [Game Environments Overview](#-game-environments-overview)
- [Game Details](#-game-details)
  - [1. Simple (Navigation)](#1-simple--navigation)
  - [2. Spread (Cooperative Coverage)](#2-spread--cooperative-coverage)
  - [3. Adversary (Deception & Inference)](#3-adversary--deception--inference)
  - [4. Push (Physical Blocking)](#4-push--physical-blocking)
  - [5. Tag (Predator-Prey Chase)](#5-tag--predator-prey-chase)
  - [6. Crypto (Encrypted Communication)](#6-crypto--encrypted-communication)
  - [7. Reference (Bidirectional Communication)](#7-reference--bidirectional-communication)
  - [8. Speaker-Listener (Unidirectional Communication)](#8-speaker-listener--unidirectional-communication)
  - [9. World Comm (Large-Scale Coordination)](#9-world-comm--large-scale-coordination)
- [Prompt Engineering](#-prompt-engineering)
- [Quick Start](#-quick-start-1)
- [Benchmark Evaluation](#-benchmark-evaluation)
- [Experiment Results](#-experiment-results)

---

## ğŸŒŸ Project Overview

This project is an LLM-based multi-agent benchmark suite built on **PettingZoo MPE (Multi-agent Particle Environment)**. It uses Large Language Models (LLMs) as the "decision brain" of agents, performing zero-shot reasoning across 9 classic multi-agent game scenarios.

**Core Idea**: Replace the traditional RL "observation â†’ neural network â†’ action" pipeline with "observation â†’ natural language prompt â†’ LLM reasoning â†’ JSON action output".

<!-- ğŸ–¼ï¸ PLACEHOLDER: Core Pipeline Diagram -->
<!-- ![Core Pipeline](./assets/pipeline_en.png) -->

### Key Features

| Feature | Description |
|:---:|:---|
| ğŸ¯ **9 Game Environments** | Covering cooperation, competition, communication, deception paradigms |
| ğŸ¤– **Multi-LLM Backend** | DeepSeek / Qwen / GPT / Gemini / Ollama / Transformers / vLLM |
| ğŸ“Š **Structured Output** | JSON logs (observation, chain-of-thought, action, reward) + MP4 video per episode |
| ğŸ§© **Modular Prompts** | Each game decomposed into 4 modules: task, physics, action format, navigation |
| ğŸ”„ **Batch Evaluation** | `benchmark_runner.py` supports N episodes Ã— multiple seeds Ã— cross-environment aggregation |

---

## ğŸ—ï¸ System Architecture

```
MPE_muiltiagent_benchmark/
â”‚
â”œâ”€â”€ ğŸ“‚ prompt/                      # Prompt modules (one file per game)
â”‚   â”œâ”€â”€ prompt_for_simple.py        #   â†’ 4 functions: task, physics, action, hints
â”‚   â”œâ”€â”€ prompt_for_spread.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“‚ obs/                         # Observation parsers (raw numpy â†’ structured dict)
â”‚   â”œâ”€â”€ parse_simple_obs.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“„ utils_api.py                 # Unified inference engine (APIInferencer)
â”œâ”€â”€ ğŸ“„ benchmark_runner.py          # Batch evaluation (N episodes Ã— 9 games)
â”‚
â”œâ”€â”€ ğŸ® simple.py                    # Game scripts Ã—9
â”œâ”€â”€ ğŸ® spread_API.py                
â””â”€â”€ ...
```

### Single-Step Execution Flow

```mermaid
graph LR
    A["ğŸŒ PettingZoo Env<br/>raw obs (numpy)"] --> B["ğŸ“ Obs Parser<br/>parse_xxx_obs()"]
    B --> C["ğŸ“ Prompt Builder<br/>user_prompt_xxx()"]
    C --> D["ğŸ¤– LLM Engine<br/>APIInferencer"]
    D --> E["ğŸ¯ JSON Action<br/>[a0, a1, a2, a3, a4]"]
    E --> F["ğŸŒ env.step(actions)"]
    F --> G["ğŸ“Š Reward + Log"]
    G --> A

    style A fill:#e1f5fe
    style D fill:#fff3e0
    style G fill:#e8f5e9
```

---

## ğŸ® Game Environments Overview

<!-- ğŸ–¼ï¸ PLACEHOLDER: Game Screenshots Collage -->
<!-- ![All Games](./assets/all_games_collage.png) -->

| # | Game | Type | Agents | Core Challenge | Comm |
|:-:|:-----|:-----|:------:|:---------------|:----:|
| 1 | **Simple** | Navigation | 1 | Single-agent navigate to landmark | âœ— |
| 2 | **Spread** | Cooperative | N (default 3) | Multi-agent landmark coverage + collision avoidance | âœ— |
| 3 | **Adversary** | Adversarial | 1 Adv + N Good | Deception: Good mislead Adv; Adv infers target | âœ— |
| 4 | **Push** | Adversarial | 1 Adv + 1 Good | Physical blocking vs. sprinting to target | âœ— |
| 5 | **Tag** | Predator-Prey | 3 Pred + 1 Prey | Cooperative hunting vs. evasion | âœ— |
| 6 | **Crypto** | Communication | Alice + Bob + Eve | Encrypted message passing vs. eavesdropping | âœ“ |
| 7 | **Reference** | Communication | 2 (dual role) | Observe partner's goal â†’ broadcast â†’ navigate | âœ“ |
| 8 | **Speaker-Listener** | Communication | 1S + 1L | Unidirectional: speaker tells target, listener navigates | âœ“ |
| 9 | **World Comm** | Large-scale | 4 Adv + 2 Prey | Leader broadcasts prey coords, hunters coordinate chase | âœ“ |

---

## ğŸ“‹ Game Details

---

### 1. Simple | Navigation

<!-- ğŸ¬ PLACEHOLDER: Simple Demo Video -->
<!-- ![Simple Demo](./assets/videos/simple_demo.mp4) -->

**Environment**: `simple_v3` | **Agents**: 1

#### ğŸ¯ Objective

Single agent navigates to a single landmark.

#### ğŸ‘ï¸ Observation Space

| Field | Dim | Description |
|:------|:---:|:------------|
| `vel` | 2 | Agent velocity `[vx, vy]` |
| `landmark_rel` | 3 | Relative position to landmark `[dx, dy, distance]` |

#### ğŸ’° Reward Function

```
reward = -â€–agent_pos - landmark_posâ€–Â² = -(dxÂ² + dyÂ²)
```

#### ğŸ•¹ï¸ Action Space

5D continuous: `[no_op, left, right, down, up]` âˆˆ [0, 1]âµ

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 2. Spread | Cooperative Coverage

<!-- ğŸ¬ PLACEHOLDER: Spread Demo Video -->

**Environment**: `simple_spread_v3` | **Agents**: N (default 3)

#### ğŸ¯ Objective

N agents cooperatively cover N landmarks while avoiding collisions.

#### ğŸ’° Reward Function

```
global = -Î£_lm min_agent dist(agent, landmark)
local  = -1.0 per collision
total  = global Ã— (1 - local_ratio) + local Ã— local_ratio
```

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 3. Adversary | Deception & Inference

<!-- ğŸ¬ PLACEHOLDER: Adversary Demo Video -->

**Environment**: `simple_adversary_v3` | **Agents**: 1 Adversary + N Good

#### ğŸ¯ Objective

- **Good Agents**: Know the target; deceive the Adversary by splitting â€” one goes to goal, another acts as decoy.
- **Adversary**: Does NOT know the target; must infer by observing Good Agents' behavior.

**Zero-sum reward**: Good wants Adv far from target; Adv wants to be close.

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 4. Push | Physical Blocking

<!-- ğŸ¬ PLACEHOLDER: Push Demo Video -->

**Environment**: `simple_push_v3` | **Agents**: 1 Adversary + 1 Good

#### ğŸ¯ Objective

- **Good Agent**: Sprint to the true target (one decoy landmark exists).
- **Adversary**: Block the Good Agent; infer which landmark is real from movement.

**Key asymmetry**: Adversary has higher mass â€” can physically push the Good Agent away.

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 5. Tag | Predator-Prey Chase

<!-- ğŸ¬ PLACEHOLDER: Tag Demo Video -->

**Environment**: `simple_tag_v3` | **Agents**: 3 Predators + 1 Prey + 2 Obstacles

#### ğŸ’° Reward Function

| Role | Condition | Reward |
|:----:|:----------|:------:|
| Predator | Catch prey | **+10.0** |
| Predator | Distance penalty | **-0.1** Ã— dist |
| Prey | Caught | **-10.0** |
| Prey | Out of bounds | **-1.0** / step |
| Prey | Survived | **+0.1** / step |

**Key asymmetry**: Prey has higher acceleration than predators.

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 6. Crypto | Encrypted Communication

<!-- ğŸ¬ PLACEHOLDER: Crypto Demo Video -->

**Environment**: `simple_crypto_v3` | **Agents**: Alice + Bob + Eve

- **Alice**: Encrypts message M with key K â†’ outputs ciphertext C
- **Bob**: Decrypts C using shared key K â†’ recovers M
- **Eve**: Intercepts C (no key) â†’ guesses M

**Action**: 4D vector âˆˆ [0, 1]â´

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 7. Reference | Bidirectional Communication

<!-- ğŸ¬ PLACEHOLDER: Reference Demo Video -->

**Environment**: `simple_reference_v3` | **Agents**: 2 (dual speaker/listener)

Each agent sees the OTHER agent's target color and must broadcast a signal to help them navigate.

**Action**: 15D = 5 (movement) + 10 (communication)

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 8. Speaker-Listener | Unidirectional Communication

<!-- ğŸ¬ PLACEHOLDER: Speaker-Listener Demo Video -->

**Environment**: `simple_speaker_listener_v4` | **Agents**: 1 Speaker + 1 Listener

- **Speaker**: Sees target (one-hot) â†’ emits 3D signal
- **Listener**: Sees landmarks â†’ receives signal â†’ navigates to target

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

### 9. World Comm | Large-Scale Coordination

<!-- ğŸ¬ PLACEHOLDER: World Comm Demo Video -->

**Environment**: `simple_world_comm_v3` | **Agents**: 4 Adversary + 2 Prey

Most complex environment with food, forests, obstacles:

| Role | Action Dim | Goal |
|:----:|:----------:|:-----|
| **Leader** | 9 (5 movement + 4 comm) | Broadcast prey coordinates |
| **Hunter** | 5 (movement) | Chase prey using leader signals |
| **Prey** | 5 (movement) | Evade hunters, reach food |

<!-- ğŸ“Š PLACEHOLDER: Results Table -->

---

## ğŸ§  Prompt Engineering

Each game's prompt is decomposed into **4 standardized modules** in `prompt/prompt_for_xxx.py`:

| Module | Function | Content |
|:-------|:---------|:--------|
| **Task & Reward** | `get_task_and_reward()` | Game rules, role objectives, reward formulas |
| **Physics** | `get_physics_rules()` | dt, damping, mass, collision detection |
| **Action Format** | `get_action_and_response_format()` | Action dimensions, JSON format, few-shot examples |
| **Navigation** | `get_navigation_hints()` | Coordinate understanding, boundary handling, strategy |

### Unified Output Format

```json
{"action": [a0, a1, a2, a3, a4], "notes": "Short Strategy"}
```

### Core Physics Parameters

| Parameter | Value | Description |
|:----------|:-----:|:------------|
| Time step `dt` | 0.1 | Simulation time per step |
| Damping | 0.25 | Velocity decay: `v â† 0.75 Ã— v` |
| Mass | 1.0 | Default agent mass |
| Sensitivity | 5.0 | Action-to-force multiplier |
| Map bounds | [-1, 1]Â² | X, Y axis range |

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure API Key

```bash
# Option A: Environment variables
export DEEPSEEK_API_KEY="sk-your-key"

# Option B: .env file (recommended)
echo 'DEEPSEEK_API_KEY=sk-your-key' > .env

# Option C: Pass at runtime
run_spread_game("deepseek", "demo.mp4", api_key="sk-your-key")
```

### 3. Run a Game

```bash
python simple.py       # Simplest entry point
python spread_API.py   # Cooperative coverage
python tag_API.py      # Predator-prey chase
```

---

## ğŸ“Š Benchmark Evaluation

```python
from benchmark_runner import run_benchmark

result = run_benchmark(
    env_name="spread",    # Any of the 9 environments
    provider="qwen",      # LLM provider
    episodes=5,           # Number of episodes
    output_dir="results/benchmarks",
)

print(f"Mean: {result['mean_reward']:.4f} Â± {result['std_reward']:.4f}")
```

---

## ğŸ“ˆ Experiment Results

<!-- ğŸ“Š PLACEHOLDER: Cross-model comparison table -->
<!--
| Environment | Qwen-3-Max | DeepSeek-Chat | GPT-4o | Gemini-1.5-Pro |
|:------------|:----------:|:-------------:|:------:|:--------------:|
| Simple | | | | |
| Spread | | | | |
| ... | | | | |
-->

<!-- ğŸ“Š PLACEHOLDER: Reward curves / Radar chart -->
<!-- ![Results Chart](./assets/charts/results.png) -->

<!-- ğŸ¬ PLACEHOLDER: Highlight replay videos -->

---

## ğŸ“Œ Placeholder Guide

This document contains the following placeholder types for you to fill in:

| Marker | Type | Where to place files |
|:-------|:-----|:--------------------|
| `ğŸ¬ PLACEHOLDER: xxx Video` | Demo video (MP4/GIF) | `docs/assets/videos/` |
| `ğŸ“Š PLACEHOLDER: xxx Table` | Results table (Markdown) | Inline in each game section |
| `ğŸ–¼ï¸ PLACEHOLDER: xxx Image` | Architecture/chart image | `docs/assets/charts/` |
| `ğŸ”— PLACEHOLDER: xxx Link` | Paper/project page link | Document header badges |

### How to Replace

1. **Videos**: Place in `docs/assets/videos/`, then uncomment the `![caption](path)` line
2. **Tables**: Copy the template in comments, fill in your data
3. **Charts**: Generate charts from benchmark JSON results, save to `docs/assets/charts/`
4. **Badges**: Update the URLs in the document header

---

<div align="center">

**Built with â¤ï¸ for Multi-Agent AI Research**

</div>
]]>
