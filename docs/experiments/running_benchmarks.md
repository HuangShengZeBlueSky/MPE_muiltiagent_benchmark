# è¿è¡Œ10æ¬¡ Adversary ç¯å¢ƒæµ‹è¯• - å®Œæ•´æŒ‡å—

## å¿«é€Ÿå¼€å§‹

```bash
cd /workspaces/MPE_muiltiagent_benchmark

# æ¿€æ´»ç¯å¢ƒ
source .venv-1/bin/activate

# è¿è¡Œ10ä¸ªepisodeçš„adversaryæµ‹è¯•
python3 benchmark_runner.py
```

## æ‰§è¡Œæµç¨‹è¯¦è§£

### ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–ï¼ˆè€—æ—¶ï¼šå‡ ç§’ï¼‰
```
âœ“ åˆ›å»ºè¾“å‡ºç›®å½•: results/benchmarks/adversary/
âœ“ å¯¼å…¥æ¸¸æˆrunner: run_adversary_game
âœ“ åˆå§‹åŒ–LLMå¼•æ“: provider="qwen"
```

### ç¬¬äºŒæ­¥ï¼šå¾ªç¯è¿è¡Œ10ä¸ªepisode

å¯¹äºæ¯ä¸ªepisodeï¼ˆi = 1 åˆ° 10ï¼‰ï¼š

#### 2.1 Episode 1 æ—¶é—´è¡¨

| é˜¶æ®µ | æ“ä½œ | è¾“å‡º | è€—æ—¶ |
|------|------|------|------|
| **åˆå§‹åŒ–** | åˆ›å»º adversary ç¯å¢ƒ | env = simple_adversary_v3(...) | 1-2s |
| **å†³ç­–å¾ªç¯** | å¾ªç¯ MAX_STEPS(5) æ­¥ | | |
| - Step 0 | 3ä¸ªGOOD agents + 1ä¸ªADVERSARY = 4ä¸ªLLMè°ƒç”¨ | 4ä¸ªaction + 4ä¸ªthought | 20-30s |
| - Step 1-4 | æ¯æ­¥4ä¸ªLLMè°ƒç”¨ | | 80-120s |
| **æ—¥å¿—è®°å½•** | ä¿å­˜step entriesåˆ°JSON | `adversary_ep1.json` (åŒ…å«20ä¸ªstep entries) | 0.5s |
| **è§†é¢‘ä¿å­˜** | ä¿å­˜5å¸§åˆ°mp4 | `adversary_ep1.mp4` | 1s |
| **æœ€ç»ˆæ±‡æ€»** | è®¡ç®—final_summary | `{"total_rewards": {"good": 8.5, "adversary": 2.1}, "mean_reward": 5.3}` | 0.1s |
| **å°è®¡** | Episode 1 | 2ä¸ªæ–‡ä»¶ä¿å­˜ | **100-150s** |

#### 2.2 Episodes 2-10ï¼ˆåŒæ ·è¿‡ç¨‹é‡å¤ï¼‰

```
Episode 2: run_adversary_game() -> adversary_ep2.{mp4,json}
Episode 3: run_adversary_game() -> adversary_ep3.{mp4,json}
...
Episode 10: run_adversary_game() -> adversary_ep10.{mp4,json}
```

**æ€»è€—æ—¶** = Episodeåˆå§‹åŒ– + 10 Ã— å•ä¸ªepisodeæ—¶é—´ = **1000-1500 ç§’** (16-25åˆ†é’Ÿ)

---

## æ—¥å¿—ç”Ÿæˆè¿‡ç¨‹

### å•ä¸ªEpisodeçš„æ—¥å¿—ç”Ÿæˆ

#### è¾“å…¥ï¼šrun_adversary_game() çš„å‚æ•°
```python
{
    "provider": "qwen",
    "output_name": "results/benchmarks/adversary/adversary_ep1",
    "N_GOOD": 3,
    "MAX_STEPS": 5
}
```

#### è¾“å‡ºï¼š`adversary_ep1.json` çš„ç»“æ„

```json
[
  // ========== Step 0 ==========
  {
    "step": 0,
    "agent": "agent_0",
    "role": "GOOD",
    "obs": {
      "role": "GOOD_AGENT",
      "goal": {"rel": [0.52, -0.38], "dist": 0.65},
      "landmarks": [
        {"id": 0, "rel": [0.52, -0.38], "dist": 0.65, "is_target": true},
        {"id": 1, "rel": [-0.48, 0.62], "dist": 0.79, "is_target": false},
        {"id": 2, "rel": [0.15, -0.85], "dist": 0.86, "is_target": false}
      ],
      "adversary": {"rel": [0.28, -0.42], "dist": 0.50},
      "teammate": {"id": "agent_1", "rel": [-0.15, 0.08], "dist": 0.17}
    },
    "action": [0.0, 0.2, 0.8, 0.0, 0.0],    // [HOLD, LEFT, RIGHT, DOWN, UP]
    "thought": "The goal is at [0.52, -0.38]. I'll move RIGHT (a[2]=0.8) towards it. The adversary is at [0.28, -0.42] which is somewhat close, but I can try to reach the goal first.",
    "reward": 0.12
  },
  {
    "step": 0,
    "agent": "agent_1",
    "role": "GOOD",
    "obs": {...},
    "action": [0.0, 0.0, 0.1, 0.5, 0.0],
    "thought": "...",
    "reward": 0.08
  },
  {
    "step": 0,
    "agent": "agent_2",
    "role": "GOOD",
    "obs": {...},
    "action": [0.0, 0.3, 0.0, 0.0, 0.7],
    "thought": "...",
    "reward": 0.15
  },
  {
    "step": 0,
    "agent": "adversary_0",
    "role": "BAD",
    "obs": {
      "role": "ADVERSARY",
      "landmarks": [
        {"id": 0, "rel": [0.52, -0.38], "dist": 0.65},
        {"id": 1, "rel": [-0.48, 0.62], "dist": 0.79},
        {"id": 2, "rel": [0.15, -0.85], "dist": 0.86}
      ],
      "good_agents": [
        {"id": "agent_0", "rel": [0.28, -0.42], "dist": 0.50},
        {"id": "agent_1", "rel": [0.15, 0.28], "dist": 0.31},
        {"id": "agent_2", "rel": [-0.12, 0.35], "dist": 0.37}
      ]
    },
    "action": [0.0, 0.0, 0.5, 0.5, 0.0],
    "thought": "The good agents are spread out. Agent 0 is closest at [0.28, -0.42]. I should move towards the landmark at [0.52, -0.38] where agent_0 is heading.",
    "reward": -0.05
  },
  
  // ========== Step 1 ==========
  {
    "step": 1,
    "agent": "agent_0",
    "role": "GOOD",
    "obs": {...},
    "action": [...],
    "thought": "...",
    "reward": 0.14
  },
  // ... (åŒæ ·æ ¼å¼ï¼ŒSteps 1-4)
  
  // ========== æœ€ç»ˆæ±‡æ€»ï¼ˆFinal Summaryï¼‰ ==========
  {
    "final_summary": true,
    "total_rewards": {
      "agent_0": 0.62,      // 5æ­¥çš„ç´¯è®¡å¥–åŠ±
      "agent_1": 0.58,
      "agent_2": 0.64,
      "adversary_0": -0.18
    },
    "mean_reward": 0.415    // (0.62 + 0.58 + 0.64 - 0.18) / 4
  }
]
```

**æ—¥å¿—ç»Ÿè®¡**ï¼š
- æ€»æ¡ç›®æ•°ï¼š4 agents Ã— 5 steps + 1 final_summary = **21æ¡**
- æ–‡ä»¶å¤§å°ï¼šçº¦ 50-100 KB (å–å†³äºLLMæ€ç»´è¿‡ç¨‹é•¿åº¦)

---

## Benchmark Runner çš„å¤„ç†

### ç¬¬ä¸‰æ­¥ï¼šè§£ææ—¥å¿—

å¯¹æ¯ä¸ª episode_i.jsonï¼š

```python
def _parse_episode_log(episode_1_json):
    # 1. æ‰¾åˆ° final_summary
    final_summary = {
        "total_rewards": {"agent_0": 0.62, "agent_1": 0.58, ...},
        "mean_reward": 0.415
    }
    
    # 2. âœ… ç›´æ¥ä½¿ç”¨ final_summary ä¸­çš„æ•°æ®ï¼ˆä¿®å¤åï¼‰
    return {
        "log_path": "results/benchmarks/adversary/adversary_ep1.json",
        "total_rewards": {"agent_0": 0.62, "agent_1": 0.58, ...},
        "mean_reward": 0.415,        # âœ… å‡†ç¡®ï¼
        "steps": 5
    }
```

### ç¬¬å››æ­¥ï¼šæ±‡æ€»ç»Ÿè®¡

```python
all_episode_stats = [
    {"episode": 1, "mean_reward": 0.415, "total_rewards": {...}},
    {"episode": 2, "mean_reward": 0.398, "total_rewards": {...}},
    ...
    {"episode": 10, "mean_reward": 0.421, "total_rewards": {...}}
]

episode_means = [0.415, 0.398, ..., 0.421]

# è®¡ç®—èšåˆç»Ÿè®¡
mean_reward = sum(episode_means) / 10 = 0.409
variance = sum((x - 0.409)^2 for x in episode_means) / 10 = 0.000125
std_reward = sqrt(0.000125) = 0.0112
```

### ç¬¬äº”æ­¥ï¼šè¾“å‡ºç»“æœ

**æ§åˆ¶å°è¾“å‡º**ï¼š
```
============================================================
ğŸ“Š BENCHMARK SUMMARY
============================================================
Environment: adversary
Provider: qwen
Episodes: 10
Mean Reward (across episodes): 0.4090
Std Dev: 0.0112
============================================================

ğŸ“ˆ Episode Statistics:

  Episode 1:
    Mean Reward: 0.4150
    Total Rewards: {'agent_0': 0.62, 'agent_1': 0.58, 'agent_2': 0.64, 'adversary_0': -0.18}
    Steps: 5

  Episode 2:
    Mean Reward: 0.3980
    Total Rewards: {'agent_0': 0.59, 'agent_1': 0.55, 'agent_2': 0.61, 'adversary_0': -0.15}
    Steps: 5

  ...

  Episode 10:
    Mean Reward: 0.4210
    Total Rewards: {'agent_0': 0.65, 'agent_1': 0.60, 'agent_2': 0.66, 'adversary_0': -0.19}
    Steps: 5

âœ… Results saved to benchmark_results.json
```

**æ–‡ä»¶è¾“å‡º**ï¼š`benchmark_results.json`
```json
{
  "env": "adversary",
  "provider": "qwen",
  "episodes": 10,
  "mean_reward": 0.409,
  "std_reward": 0.0112,
  "episode_stats": [
    {
      "episode": 1,
      "env": "adversary",
      "log": "results/benchmarks/adversary/adversary_ep1.json",
      "video": "results/benchmarks/adversary/adversary_ep1.mp4",
      "mean_reward": 0.415,
      "total_rewards": {"agent_0": 0.62, ...},
      "steps": 5
    },
    ...
  ]
}
```

---

## è¾“å‡ºæ–‡ä»¶ç»“æ„

10ä¸ªepisodeåï¼Œç”Ÿæˆçš„æ–‡ä»¶ï¼š

```
results/benchmarks/
â”œâ”€â”€ adversary/
â”‚   â”œâ”€â”€ adversary_ep1.mp4          (5å¸§è§†é¢‘)
â”‚   â”œâ”€â”€ adversary_ep1.json         (21æ¡æ—¥å¿—)
â”‚   â”œâ”€â”€ adversary_ep2.mp4
â”‚   â”œâ”€â”€ adversary_ep2.json
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ adversary_ep10.mp4
â”‚   â””â”€â”€ adversary_ep10.json
â”‚
â””â”€â”€ benchmark_results.json         (æ±‡æ€»ç»Ÿè®¡)

æ€»è®¡ï¼š
- 20ä¸ªæ–‡ä»¶ï¼ˆ10ä¸ªmp4 + 10ä¸ªjsonï¼‰
- æ€»å¤§å°ï¼šçº¦ 500-1000 MBï¼ˆæ¯ä¸ªè§†é¢‘ 50-100 MBï¼‰
- æ—¥å¿—æ€»è¡Œæ•°ï¼šçº¦ 210 æ¡entryï¼ˆ10 episodes Ã— 21 entriesï¼‰
```

---

## å…³é”®å‚æ•°è¯´æ˜

### æ—¥å¿—ä¸­çš„å…³é”®å­—æ®µå«ä¹‰

| å­—æ®µ | å«ä¹‰ | Adversaryç¤ºä¾‹ |
|------|------|-------------|
| `step` | æ¸¸æˆæ­¥æ•° (0-4) | 0 |
| `agent` | æ™ºèƒ½ä½“ID | "agent_0" æˆ– "adversary_0" |
| `role` | è§’è‰²ç±»å‹ | "GOOD" æˆ– "BAD" |
| `obs` | è§£æåçš„è§‚æµ‹ | {"goal": {...}, "landmarks": [...]} |
| `action` | è¿ç»­åŠ¨ä½œå‘é‡ | [0.0, 0.2, 0.8, 0.0, 0.0] |
| `thought` | LLMçš„æ€è€ƒè¿‡ç¨‹ | "The goal is at..." |
| `reward` | è¯¥æ­¥çš„å¥–åŠ± | 0.12 |
| `final_summary` | æ¸¸æˆç»“æŸæ ‡å¿— | true |
| `total_rewards` | æ‰€æœ‰agentsçš„ç´¯è®¡å¥–åŠ± | {"agent_0": 0.62, ...} |
| `mean_reward` | æ¸¸æˆçš„å¹³å‡å¥–åŠ± | 0.415 |

---

## å¸¸è§é—®é¢˜ & æ•…éšœæ’é™¤

### Q1: æµ‹è¯•éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ
**A**: æŒ‰ç…§ä»¥ä¸‹ä¼°ç®—ï¼š
- å•ä¸ªepisodeï¼š2-3åˆ†é’Ÿï¼ˆå«4ä¸ªLLMè°ƒç”¨Ã—5æ­¥ï¼‰
- 10ä¸ªepisodesï¼š20-30åˆ†é’Ÿ
- å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹(Ollama/Transformers)ï¼šå¯èƒ½æ›´å¿«ï¼ˆGPUåŠ é€Ÿï¼‰

### Q2: å¦‚ä½•ä¿®æ”¹MAX_STEPSï¼ˆæ­¥æ•°ï¼‰ï¼Ÿ
**A**: å½“å‰æ‰€æœ‰ç¯å¢ƒçš„MAX_STEPSæ˜¯ç¡¬ç¼–ç çš„ã€‚è¦ä¿®æ”¹ï¼Œåœ¨å¯¹åº”çš„æ¸¸æˆæ–‡ä»¶ä¸­ä¿®æ”¹ï¼š
```python
# adv_API.py
MAX_STEPS = 5  # â†’ æ”¹ä¸ºä½ æƒ³è¦çš„æ­¥æ•°
```

### Q3: æ—¥å¿—ä¸­çš„ `mean_reward` æ˜¯æ€ä¹ˆç®—çš„ï¼Ÿ
**A**: å¯¹äºAdversaryï¼š
```
mean_reward = (agent_0.reward + agent_1.reward + agent_2.reward + adversary_0.reward) / 4
            = (0.62 + 0.58 + 0.64 - 0.18) / 4
            = 0.415
```
æ³¨æ„ï¼šå¥½äººå¥–åŠ±ä¸ºæ­£ï¼Œåäººå¥–åŠ±ä¸ºè´Ÿï¼ˆé›¶å’Œæ¸¸æˆï¼‰

### Q4: å¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰è¿è¡Œï¼Ÿ
**A**: ä¿®æ”¹benchmark_runner.pyçš„mainï¼š
```python
result = run_benchmark(
    env_name="adversary",
    provider="ollama",
    episodes=10,
    output_dir="results/benchmarks",
    model_name="qwen2.5:7b"
)
```

### Q5: è§†é¢‘å¤ªå¤§ï¼Œå¦‚ä½•å‡å°æ–‡ä»¶å¤§å°ï¼Ÿ
**A**: ä¿®æ”¹å¯¹åº”æ¸¸æˆæ–‡ä»¶çš„ `imageio.mimsave()` å‚æ•°ï¼š
```python
# é™ä½FPSæˆ–è´¨é‡
imageio.mimsave(vid_name, frames, fps=2, macro_block_size=2)
```

---

## æ•°æ®åˆ†æå»ºè®®

### æå–10ä¸ªepisodeçš„ç»Ÿè®¡è¶‹åŠ¿

```python
import json
import numpy as np

with open("benchmark_results.json") as f:
    data = json.load(f)

# æå–æ¯ä¸ªepisodeçš„mean_reward
means = [s["mean_reward"] for s in data["episode_stats"]]

# è®¡ç®—è¶‹åŠ¿
print(f"Min: {min(means):.4f}")
print(f"Max: {max(means):.4f}")
print(f"Mean: {np.mean(means):.4f}")
print(f"Std: {np.std(means):.4f}")

# æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›è¶‹åŠ¿
print(f"Episode 1-5 avg: {np.mean(means[:5]):.4f}")
print(f"Episode 6-10 avg: {np.mean(means[5:]):.4f}")
```

---

