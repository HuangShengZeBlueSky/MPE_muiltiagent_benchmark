# Copilot Instructions

- **Project scope**: Multi-Agent Particle Environment benchmark driven by LLM policies. Core entrypoints per game live in the repo root (e.g., [adv_API.py](adv_API.py), [spread_API.py](spread_API.py), [reference.py](reference.py), [world_comm.py](world_comm.py)). Batch evaluation runs through [benchmark_runner.py](benchmark_runner.py) and saves mp4/json under results/benchmarks/<env>/.
- **Standardization rules**: Follow [WORKFLOW_STANDARDIZATION.md](WORKFLOW_STANDARDIZATION.md): prompts are split into exactly four functions (`get_task_and_reward`, `get_action_and_response_format`, `get_physics_rules`, `get_navigation_hints`) exported via `__all__` inside [prompt/prompt_for_*.py](prompt/) files; observation parsing lives in [obs/parse_*_obs.py](obs/) and must be runnable standalone with a sample probe; game files should only compose these pieces plus `get_api_engine`.
- **LLM engine**: [utils_api.py](utils_api.py#L1-L390) provides `APIInferencer` and `get_api_engine(provider, **kwargs)`; supported providers: openai/gpt, deepseek, qwen (base URL https://realmrouter.cn/v1), gemini, transformers, ollama, vllm. Use `get_api_engine` instead of constructing clients directly.
- **API keys & dotenv**: `.env` is auto-loaded in [utils_api.py](utils_api.py#L1-L30) and [benchmark_runner.py](benchmark_runner.py#L8-L25) when `python-dotenv` is installed. Expected vars: `OPENAI_API_KEY`, `QWEN_API_KEY`, `GOOGLE_API_KEY` (Gemini), optional `OPENAI_BASE_URL`. Ensure virtualenv has `python-dotenv`; otherwise keys must be exported manually.
- **Prompts**: Each environment’s prompt module (e.g., [prompt/prompt_for_reference.py](prompt/prompt_for_reference.py)) contains the four standardized sections only; do not inline prompts in game files. Observation semantics are formatted in the game’s obs formatter, not inside prompt modules.
- **Observation parsing**: Parsers in [obs/parse_*_obs.py](obs/) map raw PettingZoo vectors to JSON with roles, distances, directions, and tactical hints; shared helpers are in [obs/utils.py](obs/utils.py). Validate parsers via their `__main__` probes (e.g., `python obs/parse_adv_obs.py`) and follow the JSON field standards in [OBS_PARSING_GUIDE.md](OBS_PARSING_GUIDE.md).
- **Running games**: Single episodes: `python reference.py` / `python speaker_listener.py` / `python world_comm.py` etc. Benchmarks: `python benchmark_runner.py` with `env_name`, `provider`, `episodes`, optional `seed_start`; outputs mp4/json are auto-discovered by prefix and summarized with mean/std rewards.
- **Artifacts**: Games write video and log pairs with unique base names (see `get_unique_filename` in [utils_api.py](utils_api.py#L50-L70)). Benchmark runner prefixes files with `<env>_epN` under results/benchmarks/<env>/.
- **Patterns to preserve**: Keep code ASCII; avoid adding helper functions to prompt modules; keep retry/error handling inside `APIInferencer.generate_action`; maintain seed reproducibility via the `seed` argument in runners and `seed_start` in benchmarks.
- **Common pitfalls**: Missing `python-dotenv` leads to `api_key=None`; install in the active venv. Gemini currently uses deprecated `google.generativeai`; expect deprecation warnings. Qwen uses OpenAI-compatible client; ensure `QWEN_API_KEY` and base URL are set.
- **Extending**: For a new environment, first create `prompt/prompt_for_<env>.py` with the four sections, then `obs/parse_<env>_obs.py` with probe tests, and finally a `<env>_API.py` main that wires `parse_<env>_obs`, prompt builders, and `get_api_engine`.
